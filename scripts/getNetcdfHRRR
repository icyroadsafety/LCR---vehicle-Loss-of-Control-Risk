#!/usr/bin/python3
import os, sys, errno, time, argparse, re, calendar, itertools as it
import multiprocessing as mp
from joblib import Parallel, delayed
from os import path as path
from time import gmtime, mktime
import datetime
from datetime import date, timedelta
import glob
import re
import ftplib
import pytz
# from icecream import ic
#----------------------------------------------------------------------
#
fRemoteFileList_6=["a-6hour-hrrr-bfp-archive.shtml",  "a-6hour-hrrr-bfp.shtml", "a-6hour-hrrr-lcr-archive.shtml",  "a-6hour-hrrr-lcr.shtml", "a-last-run-time.shtml"]
fRemoteFileList_24=["a-24hour-hrrr-bfp-archive.shtml",  "a-24hour-hrrr-bfp.shtml", "a-24hour-hrrr-lcr-archive.shtml",  "a-24hour-hrrr-lcr.shtml", "a-24last-run-time.shtml"]
rHourList = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]
fcHourList = [6, 12, 18, 24, 30, 36, 42, 48, 54,60,66, 72]
gfsTimeDelay = 300 # minutes after initial time before grib2 becomes available.


__prog__ = os.path.basename(__file__)

# looking for the root directory of scripts - this is lcrBase
lcrBase= os.path.dirname(os.path.dirname(__file__))

# lcrBase = "/projects/me/ICY/LCR---vehicle-Loss-of-Control-Risk"

lcrLogBase =  path.join(lcrBase, "log")
lcrBinPath = path.join(lcrBase, "scripts")
lcrPngPath = path.join(lcrBase, "png")


logFile = path.join(lcrLogBase, "hrrr-download.log")
log=open(logFile,"a")

gfsPath = path.join(lcrBase, "hrrr_nc_data")


nomadsServer = "http://nomads.ncep.noaa.gov:80"
hrrrRemotePath = "/dods/hrrr"




bVerbose=0

##############################################################################################################
def doUpload(fList,type=None,overWrite=False):
    
    fListRemote =[]
    
    try:
      # danrobinson@ftp3.ftptoyoursite.com:/www.icyroadsafety.com/web/content/lcr/forecast
      session = ftplib.FTP('ftp3.ftptoyoursite.com','danrobinson','YXL@icY!VZsM526d')
      session.cwd('/www.icyroadsafety.com/web/content/lcr/forecast')
    except BaseException as exception:
      logWrite(exception)
      return 0


    if ( type and not overWrite ):     
      fListRemote=session.nlst("*."+type)
      fListRemote.sort()


    # fList=glob.glob(lcrPngPath+"/*[0-9]*.png")
    cnt=0;

    for rFile in fList:
      rClientFile=os.path.basename(rFile)
      if( overWrite == False and   rClientFile in fListRemote):
         continue

      try:
        ncfile = open(rFile,'rb')                  # file to send
        session.storbinary('STOR '+ rClientFile , ncfile)     # send the file
        ncfile.close()                                    # close file and FTP
      except BaseException as exception:
        logWrite(exception)
        session.quit()
        return cnt

      cnt+=1



    session.quit()

    # logWrite("doUpload() - uploaded %d files."% cnt)
    return cnt


def prepUpload_6( Model, runDate ):

    fList=[]

    runDateStr=runDate.strftime("%Y%m%d%H")

    scratchPath=path.join(lcrBase,"scratch")

    lcrPngList=glob.glob(lcrPngPath+"/hrrr_sfc.r{0}_[0-9]_[0-9]_lcr.png".format(runDateStr))
    bfpPngList=glob.glob(lcrPngPath+"/hrrr_sfc.r{0}_[0-9]_[0-9]_bfp.png".format(runDateStr))

    if( len(lcrPngList)==0 or len(bfpPngList)==0 ):
        return []

    linkSkeleton='<a href="/lcr/forecast/{0}"><img src="/lcr/forecast/{0}" style="width:100%;"></a>'
    with open(path.join(scratchPath,"a-6hour-hrrr-lcr.shtml"), "w" ) as f:
        f.write(linkSkeleton.format(os.path.basename(lcrPngList[0])))

    with open(path.join(scratchPath,"a-6hour-hrrr-bfp.shtml"), "w" ) as f:
        f.write(linkSkeleton.format(os.path.basename(bfpPngList[0])))

   # run date USA style month/day/year?
    with open(path.join(scratchPath,"a-last-run-time.shtml"), "w" ) as f:
        f.write(runDate.strftime("%Hz %m/%d/%Y"))



    Yesterday=datetime.datetime.now() - timedelta(days=1)

    fList+= globToFile("/hrrr_sfc.r[0-9]*_[0-9]_[0-9]_lcr.png", "a-6hour-hrrr-lcr-archive.shtml", '<a href="/lcr/forecast/{}">{}z HRRR</a><br>\n', Yesterday )
    fList+= globToFile("/hrrr_sfc.r[0-9]*_[0-9]_[0-9]_bfp.png", "a-6hour-hrrr-bfp-archive.shtml", '<a href="/lcr/forecast/{}">{}z HRRR</a><br>\n', Yesterday)

    return fList


def prepUpload_24( Model, runDate ):

    fList=[]

    runDateStr=runDate.strftime("%Y%m%d%H")

    scratchPath=path.join(lcrBase,"scratch")

    lcrPngList=glob.glob(lcrPngPath+"/hrrr_sfc.r{0}_[0-9]*_[0-9][0-9]_lcr.png".format(runDateStr))
    bfpPngList=glob.glob(lcrPngPath+"/hrrr_sfc.r{0}_[0-9]*_[0-9][0-9]_bfp.png".format(runDateStr))

    if( len(lcrPngList)==0 or len(bfpPngList)==0 ):
        return []

    linkSkeleton='<a href="/lcr/forecast/{0}"><img src="/lcr/forecast/{0}" style="width:100%;"></a>'
    with open(path.join(scratchPath,"a-24hour-hrrr-lcr.shtml"), "w" ) as f:
        f.write(linkSkeleton.format(os.path.basename(lcrPngList[0])))

    with open(path.join(scratchPath,"a-24hour-hrrr-bfp.shtml"), "w" ) as f:
        f.write(linkSkeleton.format(os.path.basename(bfpPngList[0])))

   # run date USA style month/day/year?
    with open(path.join(scratchPath,"a-24last-run-time.shtml"), "w" ) as f:
        f.write(runDate.strftime("%Hz %m/%d/%Y"))


    # 4 files in 24 hours so go back six days ?
    BackDate=datetime.datetime.now() - timedelta(days=6)

    fList+= globToFile("/hrrr_sfc.r[0-9]*_[0-9]_[0-9][0-9]_lcr.png", "a-24hour-hrrr-lcr-archive.shtml", '<a href="/lcr/forecast/{}">{}z HRRR</a><br>\n', BackDate )
    fList+= globToFile("/hrrr_sfc.r[0-9]*_[0-9]_[0-9][0-9]_bfp.png", "a-24hour-hrrr-bfp-archive.shtml", '<a href="/lcr/forecast/{}">{}z HRRR</a><br>\n', BackDate)

    return fList

def globToFile(globPattern, ArchiveFile, TextSkeleton, BackDate):
    scratchPath = path.join(lcrBase, "scratch")
    fList = []
    PngList=glob.glob(lcrPngPath+globPattern)
    if len(PngList) >0 :
        ArchiveFileHandle=open(path.join(scratchPath,ArchiveFile), "w" )
        PngList.sort( reverse=True )

        for Png in PngList:
            PngBase=os.path.basename(Png)
            [model, dt]=mkmodeldate(PngBase)
            if( dt> BackDate ):
                fList.append(Png)
                ArchiveFileHandle.write( TextSkeleton.format( PngBase, dt.strftime("%Y/%m/%d %H") ))

    return fList




def Upload(model, runDate):
    
    Debug=True
    cnt=0
    scratchPath=path.join(lcrBase,"scratch")

    if ( model == None or runDate == None ):
        return 0


    fList=prepUpload_6(model,runDate)
    fList+= prepUpload_24(model,runDate)
    if ( len(fList)==0):
        logWrite("prepUpload: No lcr/bfp images with Date {}".format(runDate))
        return 0

    # don't overwrite if png exists
    if (Debug):
        cnt=len(fList)
        print(fList);
    else:
        cnt=doUpload(fList,"png", False)

    logWrite("Upload() - uploaded %d png files."% cnt)

    # cnt=1

    # if at least 1 new image has been uploaded then also upload the a*.shtml files
    if ( cnt == 0):
       return 0

    fList=[]

    for fl in fRemoteFileList_6+ fRemoteFileList_24:
        fl=os.path.join(  scratchPath,fl )
        if(  os.path.exists(fl) ):
            fList.append(fl)

    if (Debug):
        cnt=len(fList)
        print(fList)
    else:
        cnt=doUpload(fList,"shtml", True)


    logWrite("Upload() - uploaded %d shtml files."% cnt)

    return cnt

def mkdate(datestr):
    return datetime.datetime.strptime(datestr, '%Y-%m-%d')

def mklongdate(datestr):
    return datetime.datetime.strptime(datestr, '%Y%m%d%H')

def mkmodeldate(ncFile):

    if( ncFile == None ):
        return [None, None]

    ncFileBase = os.path.basename(ncFile)

    idx=ncFileBase.find("_")
    Model=ncFileBase[0:idx]

    idx=ncFileBase.find(".")
    #tmp=baseFileName[idx+2:idx+12]
    # dt=datetime.datetime.strptime(ncFileBase[idx+2:idx+12],"%Y%m%d%H")
    dt=mklongdate(ncFileBase[idx+2:idx+12])

    return [Model, dt ]
def getGFSTimeABS(gfsStart, fcHour):
    gfsTimeABS = gfsStart + timedelta(hours=fcHour)
    return gfsTimeABS

def getGFSTimeEpoch(gfsDateTime):
#    print gfsDateTime
    gfsTimeEpoch = calendar.timegm(gfsDateTime.timetuple())
    return gfsTimeEpoch

#####
# This one is the epoch used to distinguish seemingly duplicate file names; NOT to be confused with GFSTimeEpoch.
def epochVal(gH):
    if (gH < 4): gH += 24
    return (((gH -4) / 6) * 6 + 4)

#####
# Return the most-recent GFS Date-Hour of grib2 file that *should* be available,
# or return GFS Date-Hour specified by --rdate --rhour, rounded to earlier run if needed ...
def latestGFSStart(dt=None, hr=None):
    if (dt == None): dt = datetime.fromtimestamp(mktime(gmtime()))
    if (hr == None): hr = dt.hour
    ### @note: this works assuming gfsTimeDelay < 6 hours ...
    gfsStartDate = datetime.datetime.now() - timedelta(minutes=gfsTimeDelay)
    gfsStartHour = max([h for h in rHourList if h <= gfsStartDate.hour])
    gfsStart = datetime.datetime.combine(gfsStartDate, datetime.time(gfsStartHour))
    pStart = datetime.datetime.combine(dt, datetime.time(hr))
    if (pStart < gfsStart):
        return pStart
    else:
        return gfsStart


def logWrite(sMessage):
     d1=datetime.datetime.now()
     d1.replace(microsecond=0)         

     
     sText="%s(%s): %s"%(__prog__,d1.strftime("%m-%d %H:%M"), sMessage)
     bVerbose and print(sText)

     log.write(sText+"\n")       
  
       
# return full filename of  latest download
def latestGoodDownload(Model):
    # flist=glob.glob(gfsPath+"/hrrr_sfc.r??????????.nc")
    fList=glob.glob(gfsPath+"/hrrr_sfc.r[0-9]*.nc")
    fList.sort()

    return fList.pop();
    
    
    if( len(fList) >0):
       fname=os.path.basename(fList.pop());
        # name of the form gfs.0p50.2015040500.nc     
       # we just want the date+hour    
       Ts=fname[-13:-3]
       # lDate=datetime.datetime.utcfromtimestamp(float(sT))
       lDate=datetime.datetime.strptime(Ts,"%Y%m%d%H")
       return lDate  
    else:
       return None;       

def listTodayYesterday():
     listDates=[]
     Today=datetime.datetime.today();
     Today=Today.replace(hour=0,minute=0,second=0,microsecond=0)
     Yesterday=Today-timedelta(days=1)
     Dbyday=Today-timedelta(days=2)      

     for rH in rHourList:  
       listDates.append( Today+timedelta(hours=rH) )    
       listDates.append(Yesterday+timedelta(hours=rH) );
       listDates.append(Dbyday+timedelta(hours=rH) );

     listDates.sort()
     listDates.reverse()    
     # print listDates
     return listDates;   

# finds latest live gfs data on the server
# does this by check for existance of a .das file 
def getRemoteContent(iDate,ext,numProc=1):
  Debug=False
  sDate=iDate.strftime("%Y%m%d"); # string of date
  sHour=iDate.strftime("%H");     # string of hour  

  fullRemotePath=nomadsServer+hrrrRemotePath+"/hrrr"+sDate+"/hrrr_sfc.t"+sHour+"z";
  # only append ext if not nc
  if( ext != "nc" ):
    fullRemotePath += "."+ext;

  # fullLocalPath=gfsPath+"/gfs.0p50."+sDate+sHour+"."+ext;
  epochSeconds=time.mktime(iDate.timetuple())
  
  fullLocalPath= "{}/hrrr_sfc.r{}{}.{}".format(gfsPath,sDate,sHour, ext)
  
  # this string is used by ncl to rebase the time coo-rdinates
  # from "days since 1-1-1" to "hours since filetimestamp
  time_units2="hours since {0}".format(iDate.strftime("%Y-%m-%d %H:00"))

  time_opt="small"
  # temporary test for download whole forecast at 0/6/12/18 hours - nb at these hours fcHour goes to  49 hours
  if ( int(sHour) % 6 ==0 ):
      time_opt="all"

  if(Debug): time_opt="tst"

  shellCMD= path.join(lcrBinPath,"getNetcdfData.sh")   
  
  fullCMD = "{0} {1} {2} {3} {4} {5}".format(shellCMD, fullRemotePath, fullLocalPath,ext, time_opt, "hrrr")

  ret=os.system(fullCMD);
  ret=os.waitstatus_to_exitcode(ret)
  
  if(ret==0 and os.path.isfile(fullLocalPath) ):
      return fullLocalPath
    # return os.path.basename(fullLocalPath);

  else:
    return None;   

# rebase the lon co-ordinate var from 0.0/355.0  to -180/179.5
def rebaseLocalContent(fullName):

   # fullName=gfsPath+"/"+localName
   shellCMD= lcrBinPath+"/lon360TO180.sh"
   shellCMD+= " "+ fullName
   ret=1

   if( os.path.isfile(fullName)):
      if (os.system(shellCMD)==0): 
        logWrite("Have rebased the lon coordinate of the file  {0} ".format(fullName))                        
   else:
      logWrite("unable to locate the file {0} ".format(fullName))                        

  
   return 0;

def Caption (ncFile, srtIndex, endIndex):

    # time step in hours - should be getting this from the ncFile
    step=1;

    [Product,fcDate]=mkmodeldate(ncFile)
    Product=Product.upper();


    srtDate=fcDate.astimezone(pytz.timezone('US/Central')) + datetime.timedelta(hours=srtIndex*step)
    endDate=fcDate.astimezone(pytz.timezone('US/Central')) + datetime.timedelta(hours=endIndex*step)


     # LCR maximum values through 10PM CDT Tuesday, December 5, 2023.  Generated from the 19z HRRR
    # BFP+ maximum values through 10PM CDT Tuesday, December 5, 2023.  Generated from the 19z HRRR
    title=" maximum values through {}  Generated from {}z {}.".format( endDate.strftime("%l%p CST %A, %B %e, %Y"),   fcDate.strftime("%H"), Product  )
    
    return title


def doDownloadCustom(pModel, pDate, bSkipDownload):


    fileUserBase="{0}_sfc.r{1}.nc".format(pModel,pDate.strftime("%Y%m%d%H") )
    fileUser=os.path.join(gfsPath,fileUserBase)

    if( os.path.exists(fileUser)):
        logWrite( "doDownloadCustom(): Requested file %s already exists."% (fileUserBase))
        return fileUser
    elif (bSkipDownload):
        logWrite( "doDownloadCustom(): Requested file %s doesn't exist locally."% (fileUserBase))
        return None

    # if here than try download
    dasfileName=getRemoteContent(pDate,"das")
    if (not dasfileName ):
        logWrite("doDownloadCustom(): Unable to download the user requested {0} file.".format(dasfileName) )
        return None

    os.remove(dasfileName)
    logWrite("doDownloadCustom(): downloading user file {0} ".format(fileUserBase))
    fileName=getRemoteContent(pDate,"nc");

    if( fileName ):
        logWrite("doDownloadCustom(): downloaded user file {0} ".format(fileUserBase))
        return fileUser
    else:
        logWrite("doDownloadCustom(): Unable to download the user requested {0} file.".format(fileUserBase) )
        return None

    return None



# does it all - checks the local and remote directories directory
def doDownload(Model="hrrr", bSkipDownload=False ):

    dasfileName=None
    dateServer=None
    dateLastGood=None
    fileLastGood=None

    fList=glob.glob(gfsPath+"/hrrr_sfc.r[0-9]*.nc")
    if (len(fList) ):
        fList.sort()
        fileLastGood=fList.pop()
        [m, dateLastGood]=mkmodeldate(fileLastGood)


    if(bSkipDownload):
        return fileLastGood


    # listDates is in reverse order by date
    listDates=listTodayYesterday();

    # find first good date
    for dt in listDates:
       dasfileName=getRemoteContent(dt,"das")
       if ( dasfileName ):
         dateServer=dt;
         os.remove(dasfileName);
         break;

       
    logWrite( "/***************** Starting hrrr_sfc download script ***********************/")     
    if(dateLastGood):   
      logWrite( "Last good download %s"% (dateLastGood.strftime("%Y/%m/%d %H:%M")) )               
    
    if(dateServer):
       logWrite( "Latest hrrr_sfc file on server is dated %s"% (dateServer.strftime("%Y/%m/%d %H:%M")) )    
    else:
       logWrite("No hrrr_sfc content from the last 48 hours found on server." )          
       return None


    if( dateLastGood and dateLastGood == dateServer ):       
       logWrite("The latest hrrr_sfc  data has already been downloaded." )            
       return fileLastGood
    elif(dateLastGood and  dateLastGood > dateServer  ):      
       logWrite("Paradoxically the latest local data is more recent than that on the server." )            
       return fileLastGood

    # if we are here then we need to download latest
    fileName=getRemoteContent(dateServer,"nc");    
    if( fileName): 
       logWrite("Just downloaded {0} the latest data.".format(os.path.basename(fileName)))
       return fileName
    else:
       logWrite("Unable to download {0} the latest data.".format(os.path.basename(dasfileName)))
       return None

    return None

def doLcr(ncFile, srtIndex, endIndex):


    Title =  Caption(ncFile, srtIndex, endIndex)
    # title has spaces so quote
    Title="'"+ Title+"'" 
    shellCMD= path.join(lcrBinPath,"doLCR.sh")
   
    # This script rquires the lcrBase directory - So that absolute file paths can be used
    # for calls to lcrmap.py and bfpmap.py
    fullCMD = "{0} {1} {2} {3} {4} {5}".format(shellCMD, lcrBase, ncFile, srtIndex, endIndex, Title);

    
    ncBase=path.basename(ncFile)
    ret=os.system(fullCMD);
    ret=os.waitstatus_to_exitcode(ret)      

    match ret:
       case 0:
         logWrite( "Post processing of file %s successful."% ncBase);
       case 1:
         logWrite( "Post processing of file %s failed. lcr.nco failed."% ncBase);
       case 2:
         logWrite( "Post processing of file %s failed. ncwa step failed."% ncBase);
       case 3:
         logWrite( "Post processing of file %s failed. lcrmap step failed."% ncBase);
       case 4:
         logWrite( "Post processing of file %s failed. bfpmap step failed."% ncBase);
       case 5:
         logWrite( "Post processing of file %s failed."% ncBase);
       case 10:
         logWrite( "Images from  %s already exist."% ncBase);

    if ret == 0 or ret == 10:
       return 1

    return 0;

##############################################################################################################
# usgage examples
# to download, process and upload - the latest data  from the command line
# getNecdfHRRR -v
# to get a specific data set from the NOMADS server use --rdata
# getNetcdfHRRR -v --rdate=2023121510
# to just process a specific data set
#  getNetcdfHRRR -v   --skip-download --skip-upload --rdate=2023121510
# to just upload the latest images to the website
# getNetcdfHRRR -v   --skip-download  --skip-processing
# to process and upload a specific data set
# getNetcdfHRRR -v   --skip-download  --rdate=2023121410
##############################################################################################################
__version__ = "1.00"
parser = argparse.ArgumentParser(prog=__prog__,
    description='Download Netcdf Files from NOAA NOMADS server. Make Map images. Upload to web-site.',
    formatter_class=lambda prog: argparse.HelpFormatter(prog,max_help_position=42,width=120))
parser.add_argument('--background', action='store_true', help='launch downloads in background (daemonize) ')
parser.add_argument('--dryrun', action='store_true', help="show command stream but don't execute it")
parser.add_argument('-f', '--fchour', nargs='+', type=int, choices=fcHourList, help='limit downloading to specified forecast-hour(s)')
parser.add_argument('-j', '--jobs', nargs='?', type=int, default=4, help='number of paralel processes')
parser.add_argument('-q', '--quiet', action='store_true', help='silent as the grave')
parser.add_argument('--rdate', nargs='?', type=mklongdate, help='Specify with a date/time an explict nc file to download. format e.g "2024112912"')
parser.add_argument('--rhour', nargs='?', type=int, choices=rHourList, help='download grib2 files fpr specified run-hour(s)')
parser.add_argument('--skip-download', action='store_true', help='assume data already downloaded; just launch the render-and-slice process.')
parser.add_argument('--skip-processing', action='store_true', help='do NOT launch the render-and-slice process after download.')
parser.add_argument('--skip-upload', action='store_true', help='do NOT upload to the server using ftp.')
parser.add_argument('-v', '--verbose', action='count', default=0, help='blather-on ...')
parser.add_argument('-V', '--version', action='version', version='%(prog)s  (version {0})'.format(__version__), help='print version info.')
##############################################################################################################
if __name__ == '__main__':
    os.nice(20) # Be VERY nice!

    args = parser.parse_args()

#    print(args)
    bDaemon = args.background
    bDryRun = args.dryrun
    pfcHour = args.fchour or fcHourList
    parallelJobCount = args.jobs
    bQuiet = args.quiet
    pDate = args.rdate
    # if (pDate == None): pDate = gmtime()
    # rDate = datetime.datetime.fromtimestamp(mktime(pDate))
    # rHour = args.rhour
    # if (rHour == None): rHour = rDate.hour
    bSkipProcessing = args.skip_processing
    bSkipDownload = args.skip_download
    bSkipUpload = args.skip_upload
    bVerbose = args.verbose
    if (bVerbose): bQuiet = False
    ncUserFile=None
    pModel="hrrr"

    #its best to be located in the gfs_data directory as in the shell script there is wget 
    # which creates a local file in the cwd- which is later renamed with an absolute filename
    os.chdir(gfsPath)

    # a user specified file
    if(pDate):
        ncUserFile=doDownloadCustom(pModel,pDate,bSkipDownload)
    else:
        ncUserFile=doDownload(pModel, bSkipDownload)
        [m, pDate]=mkmodeldate(ncUserFile)

    # should have a log message from doDownload
    if(not ncUserFile):
        sys.exit(1)

    # hard coded indices temporary hack    
    if not bSkipProcessing and ncUserFile:
        # process hourly
        cret = doLcr(ncUserFile, 2, 9)
        # run 24 hour model twice a day
        if pDate.hour ==6 or pDate.hour ==18:
            cret=doLcr(ncUserFile,2,25)

    if(not bSkipUpload):
        cret=Upload(pModel,pDate)
        
    sys.exit(0);

